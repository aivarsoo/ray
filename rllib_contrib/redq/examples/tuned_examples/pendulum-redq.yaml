# Pendulum REDQ can attain -140+ reward in 10k
# Configurations are the similar to original softlearning/sac and rllib's sac codebase
pendulum-redq:
    env: Pendulum-v1
    run: REDQ
    stop:
        timesteps_total: 30000
    config:
        framework: torch
        q_model_config:
          fcnet_activation: relu
          fcnet_hiddens: [256, 256]
        policy_model_config:
          fcnet_activation: relu
          fcnet_hiddens: [256, 256]
        tau: 0.005
        target_entropy: auto
        n_step: 1
        rollout_fragment_length: 1
        train_batch_size: 256
        ensemble_size: 4
        q_fcn_aggregator: min
        target_network_update_freq: 1
        min_sample_timesteps_per_iteration: 1000
        replay_buffer_config:
          type: MultiAgentPrioritizedReplayBuffer
        num_steps_sampled_before_learning_starts: 256
        optimization:
          actor_learning_rate: 0.0003
          critic_learning_rate: 0.0003
          entropy_learning_rate: 0.0001
        num_workers: 0
        num_gpus: 0
        metrics_num_episodes_for_smoothing: 5
